---
title: "evaluacion_5"
author: "Antonio Carrera Giralde"
date: "2024-06-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### 1. Introducción

#### Descripción General del Proyecto

Este proyecto tiene como objetivo principal aplicar técnicas de minería de datos para predecir el precio de casas en el mercado español, utilizando datos del año 2018. A través de este análisis, buscamos entender los factores que más influyen en los precios de las propiedades en diferentes ciudades de España, específicamente en Barcelona, Madrid y Valencia. El propósito es no solo predecir los precios de manera efectiva, sino también proporcionar insights que puedan ser de utilidad para inversionistas, empresas inmobiliarias y particulares interesados en el mercado de bienes raíces.

#### Objetivos del Análisis

El análisis se centrará en varias metas clave:

- **Implementación de Modelos de Data Science:** Aplicar y comparar diferentes modelos de predicción de precios basados en las técnicas discutidas en el módulo 5 del curso. Estos modelos incluyen regresiones lineales, árboles de decisión, y métodos más avanzados como máquinas de soporte vectorial y redes neuronales.

- **Evaluación Rigurosa:** Cada modelo será evaluado a través de una serie de métricas estadísticas que permitirán determinar su precisión y eficacia. Esto incluye el análisis del error cuadrático medio (RMSE), el error absoluto medio (MAE) y otros indicadores relevantes.

- **Interpretación de Modelos:** Desarrollar una comprensión profunda del modelo que mejor funcione, incluyendo la importancia y el impacto de las variables involucradas. Este aspecto es esencial para poder comunicar los resultados de una manera que sea comprensible para las personas no técnicas, especialmente en un contexto de negocios.

- **Productivización del Modelo:** Demostrar cómo el modelo seleccionado puede ser desplegado en un entorno de producción real, lo que incluye la configuración de una API que permita realizar consultas y obtener predicciones de precios en tiempo real.

A través de este proyecto, esperamos obtener un modelo robusto y confiable que no solo cumpla con los estándares académicos, sino que también sea directamente aplicable en el ámbito profesional, proporcionando así un valor añadido a los stakeholders del sector inmobiliario español.

#### Justificación del Proyecto

El mercado inmobiliario en España ha experimentado numerosas fluctuaciones, siendo crucial para los participantes del mercado tener herramientas que les permitan prever cambios y ajustar sus estrategias de inversión y desarrollo. Este proyecto no solo busca contribuir a la literatura académica sobre la predicción de precios inmobiliarios, sino también ofrecer una herramienta práctica y efectiva para quienes operan en este mercado.

Con este marco, el siguiente paso será configurar el entorno de trabajo y preparar los datos para las fases subsiguientes del análisis, asegurando que todos los recursos necesarios estén correctamente establecidos para el desarrollo eficiente del proyecto.

---

### 2. Configuración del Entorno y Carga de Datos

Este paso es crucial para asegurar que todos los recursos y datos necesarios están correctamente configurados y listos para ser utilizados en las fases subsiguientes del análisis.

#### Instalación y Carga de Datos
Para comenzar, necesitamos instalar y cargar la librería `idealista18` en R, que nos permitirá acceder a los conjuntos de datos necesarios para este examen. Aquí están los pasos y el código que vamos a utilizar:

```{r}
# Instalar la librería desde GitHub
if (!require("devtools")) install.packages("devtools")
devtools::install_github("paezha/idealista18")

# Cargar la librería
library(idealista18)

# Cargar los datos de las ciudades especificadas
data("Barcelona_Sale")
data("Madrid_Sale")
data("Valencia_Sale")
```

Esto instalará y cargará los conjuntos de datos para Barcelona, Madrid y Valencia, permitiéndonos acceder a la información de ventas de propiedades en estas importantes ciudades.

#### Exploración Inicial de los Datos
Una vez cargados los datos, es esencial realizar una exploración inicial para entender mejor las características de cada conjunto. Esto incluiría ver las primeras filas de los datos, resumir las estadísticas descriptivas y verificar la integridad de los datos:

```{r}
# Visualizar las primeras filas de cada conjunto de datos
head(Barcelona_Sale)
head(Madrid_Sale)
head(Valencia_Sale)

# Resumen estadístico
summary(Barcelona_Sale)
summary(Madrid_Sale)
summary(Valencia_Sale)
```

Esta exploración inicial nos ayudará a identificar patrones preliminares, posibles anomalías y proporcionará una comprensión fundamental de la estructura y tipo de datos con los que estaremos trabajando. Es importante destacar cualquier observación notable en este punto, como la presencia de valores faltantes o inusuales, que podría requerir atención especial en el preprocesamiento.

#### Verificación de la Consistencia de los Datos
Adicionalmente, realizaremos algunas verificaciones básicas para asegurarnos de que los datos están completos y listos para el análisis:

```{r}
# Verificar la existencia de valores NA en los conjuntos de datos
sum(is.na(Barcelona_Sale))
sum(is.na(Madrid_Sale))
sum(is.na(Valencia_Sale))

# Comprobar el tipo de datos de cada columna
str(Barcelona_Sale)
str(Madrid_Sale)
str(Valencia_Sale)
```

Estas verificaciones son esenciales para prevenir problemas en fases posteriores del análisis y asegurar que los modelos de predicción que desarrollaremos se basen en datos limpios y bien estructurados.

---

### Paso 3: Limpieza y Preprocesamiento de Datos

En esta etapa, nos centraremos en limpiar y preparar los datos para su posterior análisis y modelado. Los resultados del paso anterior indican que hay una cantidad significativa de valores faltantes (NA) y algunos aspectos de los datos que requieren normalización y corrección.

#### Limpieza de Datos
1. **Tratamiento de Valores Faltantes**: Dado que los conjuntos de datos presentan valores NA significativos en variables críticas, aplicaremos estrategias para manejar estos valores, como imputación o eliminación, dependiendo del contexto y la cantidad de datos faltantes.

```{r}
# Tratamiento de valores NA para 'CONSTRUCTIONYEAR' usando la mediana del año de construcción
Barcelona_Sale$CONSTRUCTIONYEAR[is.na(Barcelona_Sale$CONSTRUCTIONYEAR)] <- median(Barcelona_Sale$CONSTRUCTIONYEAR, na.rm = TRUE)
Madrid_Sale$CONSTRUCTIONYEAR[is.na(Madrid_Sale$CONSTRUCTIONYEAR)] <- median(Madrid_Sale$CONSTRUCTIONYEAR, na.rm = TRUE)
Valencia_Sale$CONSTRUCTIONYEAR[is.na(Valencia_Sale$CONSTRUCTIONYEAR)] <- median(Valencia_Sale$CONSTRUCTIONYEAR, na.rm = TRUE)

# Eliminación de filas con valores NA excesivos en características clave
Barcelona_Sale <- na.omit(Barcelona_Sale)
Madrid_Sale <- na.omit(Madrid_Sale)
Valencia_Sale <- na.omit(Valencia_Sale)
```

2. **Normalización de Variables Numéricas**: Las variables numéricas como el precio y el área construida variarán significativamente entre diferentes propiedades, lo que puede afectar el rendimiento del modelo. Normalizaremos estas variables para tener una escala común.

```{r}
normalize <- function(x) {
  # Extraer solo los valores finitos y no NA para calcular min y max
  finite_x <- x[is.finite(x)]
  
  # Si no hay valores finitos, devolver un vector de NA
  if (length(finite_x) == 0) {
    return(rep(NA, length(x)))
  }
  
  # Calcular mínimo y máximo de los valores finitos
  min_x <- min(finite_x, na.rm = TRUE)
  max_x <- max(finite_x, na.rm = TRUE)
  
  # Si todos los valores son iguales, devolver 0.5 o cualquier constante porque la normalización no tiene sentido
  if (min_x == max_x) {
    return(rep(0.5, length(x)))
  }
  
  # Aplicar la normalización, incluyendo el manejo de NA y valores infinitos en el vector original
  normalized_values <- (x - min_x) / (max_x - min_x)
  normalized_values[!is.finite(x)] <- NA  # Asignar NA a valores Inf/-Inf o NA originales
  
  return(normalized_values)
}

# Aplicar la función modificada a los datos
Barcelona_Sale$PRICE <- normalize(Barcelona_Sale$PRICE)

```

3. **Corrección de Tipos de Datos**: Asegurarnos de que todas las variables tengan el tipo de datos correcto, especialmente las variables categóricas que podrían haber sido interpretadas incorrectamente como numéricas.

```{r}
# Corrección de tipos de datos
Barcelona_Sale$HASLIFT <- as.factor(Barcelona_Sale$HASLIFT)
Madrid_Sale$HASLIFT <- as.factor(Madrid_Sale$HASLIFT)
Valencia_Sale$HASLIFT <- as.factor(Valencia_Sale$HASLIFT)
```

#### Análisis Exploratorio Post-Limpieza
Una vez que los datos han sido limpiados y preprocesados, es vital realizar un análisis exploratorio para asegurar que los cambios fueron efectivos y para obtener nuevas insights sobre los conjuntos de datos ajustados.

```{r}
# Visualizar resúmenes estadísticos post-limpieza
summary(Barcelona_Sale)
summary(Madrid_Sale)
summary(Valencia_Sale)

# Crear visualizaciones para comparar distribuciones
library(ggplot2)
ggplot(Barcelona_Sale, aes(x = PRICE)) + geom_histogram(bins = 30) + ggtitle("Distribución de precios en Barcelona")
ggplot(Madrid_Sale, aes(x = PRICE)) + geom_histogram(bins = 30) + ggtitle("Distribución de precios en Madrid")
ggplot(Valencia_Sale, aes(x = PRICE)) + geom_histogram(bins = 30) + ggtitle("Distribución de precios en Valencia")
```
 
 